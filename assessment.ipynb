{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14925000-fbff-4a92-a451-bf3008e8aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as Type\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d2bbf-a01a-4eed-b18b-4baf637e614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ExcelReadExample\").getOrCreate()\n",
    "excel_file_path = r'/home/glue_user/workspace/jupyter_workspace/Input_v2.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0c36a-4964-4ad4-b2e1-1e86668b981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataValidator:\n",
    "    # def __init__(self, excel_file_path):\n",
    "    #     self.file_path = excel_file_path\n",
    "    #     self.spark = SparkSession.builder.appName(\"Project1\").getOrCreate()\n",
    "\n",
    "def read_file(excel_file_path):\n",
    "    df_pd = pd.read_excel(excel_file_path, engine='openpyxl')\n",
    "    df0 = spark.createDataFrame(df_pd)\n",
    "    return df0\n",
    "\n",
    "# ----------------------- exploratory data analysis ----------------------- \n",
    "\n",
    "def data_analysis(excel_file_path):\n",
    "    df0 = read_file(excel_file_path)\n",
    "\n",
    "    # schema\n",
    "    print(f'1. data schema')\n",
    "    df0.printSchema()\n",
    "\n",
    "    # statistical data summary\n",
    "    print(f'2. statistical data summary')\n",
    "    df0.describe().show()\n",
    "\n",
    "    # check duplicates\n",
    "    print(f'3. check duplicates')\n",
    "    df0.groupBy(df0.columns).count().where(F.col('count') > 1).select(F.sum('count')).show()\n",
    "\n",
    "    print(f'4. sample data')\n",
    "    df0.show(5, truncate=False)\n",
    "\n",
    "    # check NaN / NULL / empty string\n",
    "    print(f'5. check NaN / NULL / empty string')\n",
    "    for col, data_type in df0.dtypes:\n",
    "        nan_count = df0.filter(F.isnan(F.col(col))).count()\n",
    "        null_count = df0.filter(F.col(col).isNull()).count()\n",
    "        empty_str_count = df0.filter(F.col(col)=='').count()\n",
    "\n",
    "        if (nan_count > 0) or (null_count > 0) or (empty_str_count > 0):\n",
    "            print(f\"{col} with {data_type} -> nan: {nan_count}, null: {null_count}, '': {empty_str_count}\")\n",
    "\n",
    "def general_preprocess(excel_file_path, remove_nan=False):\n",
    "    df0 = read_file(excel_file_path)\n",
    "\n",
    "    # ----------------------- data pre-processing -----------------------\n",
    "\n",
    "    # preprocess for column header\n",
    "    # 1. remove leading and trailing space\n",
    "    # 2. replace ' ' with '_'\n",
    "    # 3. change uppercase char to lowercase char\n",
    "\n",
    "    df1 = df0.select([F.col(col).alias(col.strip().replace(\" \", \"_\").lower()) for col in df0.columns])\n",
    "    \n",
    "    if remove_nan is True:\n",
    "        df1 = df1.na.drop()\n",
    "        \n",
    "    # cast double / long data type into int\n",
    "    for col, data_type in df1.dtypes:\n",
    "        if data_type != 'string':\n",
    "            df1 = df1.withColumn(col, df1[col].cast(Type.IntegerType()))\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "def eliminate_outliers(df, col_name, outlier_threshold):\n",
    "    stats_summary = df.describe(col_name)\n",
    "    mean_val = stats_summary.filter(stats_summary[\"summary\"] == \"mean\").select(col_name).collect()[0][col_name]\n",
    "    stddev_val = stats_summary.filter(stats_summary[\"summary\"] == \"stddev\").select(col_name).collect()[0][col_name]\n",
    "\n",
    "    df_with_z_score = df.withColumn(f\"{col_name}_z_score\", (F.col(col_name) - mean_val) / stddev_val)\n",
    "\n",
    "    df_final = df_with_z_score.filter(F.abs(F.col(f\"{col_name}_z_score\")) < outlier_threshold) \\\n",
    "                                .drop(f\"{col_name}_z_score\")\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2648b40-48af-4db8-9aa5-0c31b8bd834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- question 1: group_by_age function -----------------------\n",
    "\n",
    "def group_by_age(excel_file_path):\n",
    "    df0 = general_preprocess(excel_file_path, remove_nan=True)\n",
    "\n",
    "    curr_year = datetime.today().year\n",
    "\n",
    "    # get age for each person\n",
    "    df1 = df0.withColumn(\"age\", curr_year - F.col(\"birth_year\"))\n",
    "\n",
    "    # Group by age and collect names into lists\n",
    "    result_dict = (\n",
    "                df1.groupBy(\"age\")\n",
    "                .agg({\"name\": \"collect_list\"})\n",
    "                .withColumnRenamed(\"collect_list(name)\", \"names\")  # collect categorized name into a list\n",
    "                .orderBy(\"age\")\n",
    "                .select(\"age\", \"names\")\n",
    "                .rdd.collectAsMap()  # df -> rdd -> dict\n",
    "    )\n",
    "\n",
    "#     null_names = df0.filter(F.isnan(F.col(\"birth_year\"))) \\\n",
    "#         .select(\"name\") \\\n",
    "#         .rdd.flatMap(list).collect()\n",
    "\n",
    "#     result_dict['NA'] = null_names\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "result = group_by_age(excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937a4c7-8279-4b8f-8d27-c5794e3e4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- question 2: vowel in middle of the name -----------------------\n",
    "\n",
    "def check_vowel(excel_file_path):\n",
    "    df1 = general_preprocess(excel_file_path)\n",
    "\n",
    "    # Define a UDF with a lambda function\n",
    "    udf_is_vowel = F.udf(lambda name: len(name) % 2 == 1 and name[len(name)//2].lower() in ['a', 'e', 'i'], Type.BooleanType())\n",
    "\n",
    "    # Add a new column \"HasVowelInMiddle\"\n",
    "    df2 = df1.withColumn(\"vowel_in_middle\", udf_is_vowel(F.col(\"name\")))\n",
    "\n",
    "    # collect into list\n",
    "    result = df2.filter(\"vowel_in_middle == True\").select(\"name\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    return result\n",
    "\n",
    "result = check_vowel(excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b44f42-e88a-4c06-954f-4246828fb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- question 3: age category -----------------------\n",
    "\n",
    "def age_category(excel_file_path):\n",
    "    df1 = general_preprocess(excel_file_path, remove_nan=True)\n",
    "\n",
    "    curr_year = datetime.today().year\n",
    "\n",
    "    # get age for each person\n",
    "    df1 = df1.withColumn(\"age\", curr_year - F.col(\"birth_year\"))\n",
    "\n",
    "    # age group\n",
    "    df1 = df1.withColumn(\n",
    "            'AGE_GROUP',\n",
    "            F.when(F.col(\"age\").between(0, 17), 'TEENAGER')\\\n",
    "            .when(F.col('age').between(18,39), 'YOUNG')\\\n",
    "            .when(F.col('age').between(40,100), 'OLD')\n",
    "            .otherwise('NA'))\n",
    "    \n",
    "    return df1\n",
    "\n",
    "df3 = age_category(excel_file_path)\n",
    "# df3.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4e48a-f716-468e-92ff-ca2b2d443ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- question 4: check ID uniqueness  -----------------------\n",
    "def check_id_uniqueness(excel_file_path):\n",
    "    df1 = general_preprocess(excel_file_path)\n",
    "\n",
    "    if df1.count() > df1.dropDuplicates(['id']).count():\n",
    "        print('has duplicates in id')\n",
    "\n",
    "        df2 = df1.groupby(['id']) \\\n",
    "                    .count() \\\n",
    "                    .filter('count > 1') \\\n",
    "        \n",
    "        return df2\n",
    "\n",
    "    else:\n",
    "        print('no duplicate in id')\n",
    "\n",
    "        \n",
    "df = check_id_uniqueness(excel_file_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e456d-589e-4131-b609-cbe6513a50d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------- question 5: age group average  -----------------------\n",
    "\n",
    "\n",
    "def calculate_and_save_average_age(excel_file_path):\n",
    "    df3 = age_category(excel_file_path)\n",
    "\n",
    "    # partition by the age group set in previous question and find the average\n",
    "    window = Window.partitionBy(\"AGE_GROUP\")\n",
    "    avg_age = F.avg(\"age\").over(window)\n",
    "    df4 = df3.withColumn(\"AVERAGE\", F.round(avg_age, 2))\n",
    "\n",
    "    df4 = (df4.withColumnRenamed('id', 'ID')\n",
    "                    .withColumnRenamed('name', 'NAME')\n",
    "                    .withColumnRenamed('birth_year', 'BIRTHYEAR')\n",
    "                    .withColumnRenamed('age', 'AGE'))\n",
    "\n",
    "    # Get specific column and export .csv \n",
    "    df4.select(\"ID\", \"NAME\", \"BIRTHYEAR\", \"AGE\", \"AGE_GROUP\", \"AVERAGE\") \\\n",
    "             .write \\\n",
    "             .csv(\"output.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# calculate_and_save_average_age(excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e93b91-44ff-4276-93ef-d1633fca8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- question 6: get grades  -----------------------\n",
    "\n",
    "def get_grades(excel_file_path):\n",
    "    df1 = general_preprocess(excel_file_path)\n",
    "        \n",
    "    outlier_threshold = 1.5\n",
    "    df1 = eliminate_outliers(df1, 'assignment_a', outlier_threshold)\n",
    "    df1 = eliminate_outliers(df1, 'assignment_b', outlier_threshold)\n",
    "    df1 = eliminate_outliers(df1, 'mid_term_exam', outlier_threshold)\n",
    "    df1 = eliminate_outliers(df1, 'project', outlier_threshold)\n",
    "    df1 = eliminate_outliers(df1, 'final_exam', outlier_threshold)\n",
    "\n",
    "    # provided weights\n",
    "    weights = {\n",
    "        \"assignment_a\": 0.07, \n",
    "        \"assignment_b\": 0.13, \n",
    "        \"mid_term_exam\": 0.20, \n",
    "        \"project\": 0.25, \n",
    "        \"final_exam\": 0.35\n",
    "    }\n",
    "\n",
    "    # create expression for weights\n",
    "    weighted_avg_expr = sum(F.col(criteria) * percentage for criteria, percentage in weights.items())\n",
    "\n",
    "    df1 = df1.withColumn(\"GRADE\", weighted_avg_expr)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aab164-65bd-4950-bfe9-e300adc900a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- question 7: draw a chart to show the trend of student -----------------------\n",
    "\n",
    "\n",
    "def return_chart_student_performance():\n",
    "    df = pd.read_excel(excel_file_path, engine='openpyxl')\n",
    "    df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
    "\n",
    "    df1 = df.query(\"\"\"\n",
    "        assignment_a <= 100 and \\\n",
    "        assignment_b <= 100 and \\\n",
    "        mid_term_exam <= 100 and \\\n",
    "        project <= 100 and \\\n",
    "        final_exam <= 100\n",
    "    \"\"\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for index, row in df1.iterrows():\n",
    "        student_name = row['name']\n",
    "        student_scores = row[['assignment_a', 'assignment_b', 'mid_term_exam', 'project', 'final_exam']]\n",
    "\n",
    "        plt.plot(student_scores, label=student_name)\n",
    "\n",
    "    plt.xlabel('Assessment Type')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Student Performance Trend During the Semester')\n",
    "    plt.xticks(range(5), ['Assignment A', 'Assignment B', 'Mid Term Exam', 'Project', 'Final Exam'])\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "return_chart_student_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a761f0-9a92-4ba1-925b-3c9a29da241e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
